# ğŸ” Forgery Detection and Image Captioning using ELA, ViT, and BLIP-2

This repository contains a comprehensive solution for **detecting forged regions in images** and generating **context-aware captions** using:
- **Error Level Analysis (ELA)** for preprocessing,
- **Vision Transformer (ViT)** for feature extraction,
- **BLIP-2** for multimodal captioning.

---

## ğŸ“Œ Features

- ğŸ–¼ï¸ **Forgery Detection** using ELA heatmaps and ViT-based classification/localization
- ğŸ§  **Caption Generation** with BLIP-2 to describe manipulated content
- ğŸ“ˆ Supports CASIA 2.0 (training) and CASIA 1.0 (testing)
- ğŸš€ Compatible with GPU for efficient training/inference
- ğŸ“¦ Modular, clean PyTorch code

---

## ğŸ—‚ï¸ Project Structure

```bash
Forgery-Detection-and-Captioning/
â”œâ”€â”€ data/                     # Dataset loading and preprocessing
â”œâ”€â”€ models/                   # ViT + BLIP-2 model definitions
â”œâ”€â”€ utils/                    # Helper functions (metrics, ELA, visualization)
â”œâ”€â”€ output_dir/               # Training outputs (checkpoints, logs)
â”œâ”€â”€ pretrained-weights/       # Pretrained weights (ViT, BLIP-2)
â”œâ”€â”€ notebooks/                # Jupyter notebooks for demo/training
â”œâ”€â”€ train.py                  # Main training script
â”œâ”€â”€ evaluate.py               # Evaluation script
â”œâ”€â”€ config.yaml               # Configuration file
â””â”€â”€ README.md                 # You're here
